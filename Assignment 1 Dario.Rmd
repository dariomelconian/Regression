---
title: "Assignment 1"
author: "Dario Melconian"
date: "22/09/2020"
output:
  word_document: default
  pdf_document: default
---

Question 1:

a). Find the LS estimates of B0 and B1:
```{r}
Sxy = 25825

Sxx = 39095
Syy = 17454

Sx = 517
Sy = 346

num_obs = 14

meanX = Sx / num_obs
meanY = Sy / num_obs

# beta parameter estimation using least square estimation
beta_1_hat = (Sxy - num_obs*meanX*meanY) / (Sxx - num_obs*meanX^2)
beta_0_hat = meanY - (beta_1_hat * meanX)

beta_0_hat
beta_1_hat
```
Thus, beta0hat is 0.626 and beta1hat is 0.652.

b). Using the estimates, find the fitted value y at x = 120.
```{r}
y_hat = beta_0_hat + beta_1_hat * 120

y_hat
```
Therefore, the fitted value at x = 120 is 78.90.

c). Obtain unbiased estimate of sigma^2
```{r}
observedError = 391.8257
sigma_hat_squared = (1 / (num_obs - 2)) * observedError

sigma_hat_squared
```
Therefore, the unbiased estimate of sigma^2 is 32.65.

d). What proportion of observed variation in y can be explained by the linear relationship between the two variables?
```{r}
# Calculate R^2
SSE = observedError

SST = Syy - (num_obs * (meanY^2))

R_squared = 1-(SSE/SST)

R_squared
```
Please ignore the 1 d). I did on paper, that is wrong and was my initial take.  This is my correct version. 
Therefore, the proportion of observed variation in y, R^2, is 0.956. 

e). Construct 95% CI for E(Y|x=120)
```{r}
tAlpha = 2.179
x = 120

SxxValue = Sxx - (num_obs * (meanX^2))
xMinusXbar = (x - meanX)^2

# upper bound
UpperCI = (y_hat + tAlpha * (sqrt(32.6521*((1 / num_obs) + (xMinusXbar / SxxValue)))))

# lower bound
LowerCI = (y_hat - tAlpha * (sqrt(32.6521* ((1 / num_obs) + (xMinusXbar / SxxValue)))))

# create interval in the form of a vector
ConfidenceInterval = c(LowerCI, UpperCI)

# call it
ConfidenceInterval
```
Thus, I can be 95% certain that the interval (70.87, 86.94) contains the true mean of the population. 


Question 2:

Done on paper.


Question 3:
```{r}
hw1_data = read.csv("https://raw.githubusercontent.com/hgweon2/ss3859/master/hw1_data1.csv")

num_obs3 = 100
```

a). Count the number of observations whos x1 are lower than 4
```{r}
sum(hw1_data$x1 < 4)
```
Therefore, there are 40 observations whos x1 is lower than 4.

b).
```{r}
sum(hw1_data$x1 < 4 & hw1_data$x2 == "L")
```
Therefore, there are 32 observations whos x1 is lower than 4 and x2 is equal to L.

c).
```{r}
subsetA = subset(hw1_data, x2 == "L")

mean(subsetA$x1)
median(subsetA$x1)
sd(subsetA$x1)
```
Thus, mean is 2.58, median is 2.57, and standard deviation is 2.15.

d). 
```{r}
# null hypothesis: mu = 4

# alternative hypothesis: mu =/= 4

t.test(hw1_data[,1], conf.level = 0.95, mu = 4, alternative = "two.sided")
```
My conclusion yields that the p-value is indeed greater than the significance level of 0.05, so I fail to reject the null hypothesis, and must accept the null hypothesis, interpreting that it may not necessarily differ from 4.  Thus, because p-value > significance level (alpha), it is possible that mu = 4.  Therefore, cannot reject the null hypothesis.

e).
```{r}
# Ho: mu = 2
# H1: mu > 2
# one-sided test statistic

t.test(subsetA$x1, conf.level = 0.95, mu = 2, alternative = "greater")
```
The statement that "given x2 = L, the true mean of x1 is larger than 2" is convincing.  This is because the test statistic's p-value is less than the significance level of alpha, so we can reject the null hypothesis that it is = 2.  Therefore, we can conclude the true mean of x1 is larger than 2, due to the p-value being less than the signifiance level of alpha (0.05).


Question 4:
```{r}
set.seed (10)
idx = sample(nrow( cars ) ,30 , replace=FALSE) 
cars2 = cars[idx,]
```

a).  Make scatterplot for relationship between X and Y:
```{r}
plot(cars2, col = "red")
```
I obviously notice the pattern that the increase in speed (x) results in an increase in distance (y).  Positively correlated. 

b). Obtain the LS estimates for B0, B1 and sigma^2:
```{r}

# create linear model
newDataset = lm(dist~speed, data = cars2)

# summarize 
summary(newDataset)
```
Unbiased estimates for beta0hat = -18.4659, and for beta1hat = 3.9015.  Sigma^2hat = 14.

c). Using the estimates, calculate the residuals of e5, e10, e20 (rows of cars2 data):
```{r}
newDataset$residuals[c(5,10,20)]
```

d). Find the residuals whose absolute values are > 20. Indicate those residuals in scatterplot with different color/shape:
```{r}
# find out how many there are
sum(abs(newDataset$residuals) > 20)

# find out which they are and their associated values
which(abs(newDataset$residuals) > 20)

# scatterlpot with these values in different colour
idx = which(abs(newDataset$residuals) > 20)

col_idx = rep(1,nrow(cars2)) # rest of residuals are black
#col_idx

col_idx[idx] = 4  # blue residuals > 20
#col_idx

plot(dist ~ speed, data = cars2,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = col_idx,
     cex  = 1,
     col  = col_idx)
```
Therefore, the residuals whos absolute values are greater than 20 are 34, 22, and 35. 

e). Calculate the sum of the residuals:
```{r}
sum(newDataset$residuals)
```
Therefore, the sum of the residuals is -5.66 x10^15, nearly 0.

f).
```{r}
plot(dist ~ speed, data = cars2,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = col_idx) 

# add absolute line
abline(newDataset) # adds an absolute line

# predict the stopping distance at speed = x = 21
predict(newDataset, newdata = data.frame(speed = 21)) 
```
Therefore, the distance taken to stop when travelling at 21 mph is 63.46523 feet.

g).
```{r}
# from the summary of the newDataset
# R^2 = 0.6734
```
This R^2 = 0.6734 value implies that 67% of the variation in the response variable is explained by the fitted model. 

h).
The issue with this statement is that it is not designed to be an "exact" prediction of distance at any given speed.  Rather, the fitted model is a calculated approximation or an average calculated between the relationship following the regression line and its equation.  So, taking in all the data, it predicts where any y point would lie with a given x, having followed the regression equation using the estimates for beta's and variance.  It follows the formula for the fitted regression line.  It is fitted, meaning it is not exact at any given point, which is why one cannot say given any x value, there is an EXACT y value.  It is an approximation for the region where the value should lie in accordance with the fitted regression equation and line. 

i).
```{r}
confint(newDataset)
```
Thus, we can see the 95% CI for beta1 using the confint function being (2.85, 4.95).

j).
```{r}
# predicted value at the speed 
predict(newDataset, newdata = data.frame(speed = 21))
# confidence interval for that value
predict(newDataset, newdata = data.frame(speed = 21), interval = "confidence", level = 0.90)
```
Thus, we can see the 90% CI for E(Y|x=21) being (56.81, 70.12).

